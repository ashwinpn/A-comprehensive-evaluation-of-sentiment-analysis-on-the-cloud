{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "caml_rnn_module.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR5fhR6Ihgey"
      },
      "source": [
        "# Sentiment Analysis using Recurrent Neural Networks\n",
        "\n",
        "Cloud and Machine Learning Project\n",
        "\n",
        "Ashwin Prakash Nalwade (apn308), Mingxi Chen (mc7805)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U175tcBShge1"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "We will adjust the seed, define the `Fields` and maintain training, validdation, and test splits. We will focus on *packed padded sequences* - this will enable the RNN to process only those parts of the sentence which are not padded, and hence for the padded parts, the `output` would be a zero tensor. For leveraging packed padded sequences, the RNN has to be aware of the length of the actual sentences . We achieve this by adjusting the `include_lengths` parameter, setting it as `True`.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0_tTv-hhge3"
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbosrQjdhgfI"
      },
      "source": [
        "Load the IMDB dataset now.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKRjrB_5hgfI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a95e7b-67b5-413b-e1a2-abd5a1dc3a9e"
      },
      "source": [
        "from torchtext import datasets\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:08<00:00, 9.85MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOE7VtvYhgfJ"
      },
      "source": [
        "Split, validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVPOh0dzhgfJ"
      },
      "source": [
        "import random\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPheBMKahgfJ"
      },
      "source": [
        "Now we use word embeddings. We take the `glove.6B.100d` vectors. `glove` is an algorithm developed at Stanford, used to calculate the vectors. Here, '100d' indicates the fact that the vectors have 100 dimensions, and `6B` implies that they were trained on six billion tokens. \n",
        "\n",
        "\n",
        "**Note**: The size of glove is significantly large  (~862MB), so keep a track of the internet connectivity.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT1g-NyRhgfJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0a93589-2dc4-425c-ac09-95da2346c87c"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:31, 2.20MB/s]                          \n",
            " 99%|█████████▉| 397660/400000 [00:16<00:00, 26039.11it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by56X4m8hgfJ"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z9ZfBqbhgfJ"
      },
      "source": [
        "## Building the Model\n",
        "\n",
        "### RNN Architecture\n",
        "\n",
        "We make use of a RNN type called as a Long Short-Term Memory (or LSTM, as it is commonly known). LSTM's do not face the same issues that normal RNN's usually deal with, such s the vanishing gradient problem. LSTM's prevent this issue by including an extra auxillary state called as a _cell_, $c$ - which could be considered analogous to the \"memory\" part of the LSTM. They also include   multiple _gates_ that co-ordinate the flow of information from and to the memory. We can consider the LSTM as a function of $h_t$, $x_t$, and $c_t$. \n",
        "\n",
        "Thus,\n",
        "\n",
        "$$(h_t, c_t) = \\text{LSTM}(x_t, h_t, c_t)$$\n",
        "\n",
        "The model looks similar to this:\n",
        "\n",
        "![](https://github.com/ashwinpn/sentiment_analysis/blob/main/sentiment2.png?raw=1)\n",
        "\n",
        "### Bidirectional RNN\n",
        "\n",
        "Now, we can have a recurrent neural net that is processing the input text from left to right (forward recurrent neural net), and also a second RNN that processes the input from right to left (backward recurrent neural net). At a given time instant $t$, the forward net is working on $x_t$, while the backward net is working on $x_{T-t+1}$. \n",
        "\n",
        "In the following figure, the forward net is in orange, and the backward net is in green:   \n",
        "\n",
        "![](https://github.com/ashwinpn/sentiment_analysis/blob/main/sentiment3.png?raw=1)\n",
        "\n",
        "### Multi-layer RNN\n",
        "\n",
        "In this case, we have multiple recurrent neural nets on the top of the first RNN - each RNN can basically be considered as a layer. The output of the first RNN feeds as an input to the RNN stacked aboce it. \n",
        "\n",
        "![](https://github.com/ashwinpn/sentiment_analysis/blob/main/sentiment4.png?raw=1)\n",
        "\n",
        "### Regularization\n",
        "\n",
        "Having a large number of parameters in the model implies that we\n",
        "would have a larger probability of the occurrence of overfitting [That is, train too closely\n",
        "on the training data, leading to a high training accuracy and low train error BUT lower\n",
        "test, validation accuracies and low test, validation errors]. Thus, regularization is crucial\n",
        "to prevent this from occurring. There are various regularization methods like lasso / ridge\n",
        "regression, (l1+l2) regression, but we use dropout. Dropout operates by randomly\n",
        "deleting neurons within a layer in a forward pass.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsRcElpMhgfJ"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.rnn = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        #pack sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu())\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        \n",
        "        #unpack sequence\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * num directions]\n",
        "        #output over padding tokens are zero tensors\n",
        "        \n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\n",
        "        #cell = [num layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "        #and apply dropout\n",
        "        \n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "                \n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "            \n",
        "        return self.fc(hidden)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLlRg5h3hgfJ"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = RNN(INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM, \n",
        "            N_LAYERS, \n",
        "            BIDIRECTIONAL, \n",
        "            DROPOUT, \n",
        "            PAD_IDX)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuLUsAa6hgfK",
        "outputId": "61ca5758-4a65-42dd-a67b-6392fa51cb6d"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 4,810,857 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVs3EPYkhgfn",
        "outputId": "dfe8d92a-dc5b-439b-88da-416fc0970759"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([25002, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0m1wQ0jhgfo",
        "outputId": "64fb3b66-4d27-49e5-fb42-ccd4d339e5bd"
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
              "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [ 1.0406,  0.2109, -0.6788,  ...,  0.9281,  0.1573,  0.9135],\n",
              "        [-0.0259,  0.2626, -0.4150,  ...,  0.2496,  1.0473, -0.8566],\n",
              "        [-0.7507,  0.0280,  0.4090,  ..., -0.0273,  0.3827,  0.3968]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SDCUegXhgft",
        "outputId": "a774ea6d-4665-4c7f-a2ee-d1a57aadbb7c"
      },
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [ 1.0406,  0.2109, -0.6788,  ...,  0.9281,  0.1573,  0.9135],\n",
            "        [-0.0259,  0.2626, -0.4150,  ...,  0.2496,  1.0473, -0.8566],\n",
            "        [-0.7507,  0.0280,  0.4090,  ..., -0.0273,  0.3827,  0.3968]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-cuC_qchgfu"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "499flZLkhgfu"
      },
      "source": [
        "\n",
        "We will be using the `Adam` optimizer here. The `Adam` optimizer adjusts the leanring rate for all the parameters, attrubuting lower learning rates to the parameters that are updated with more frequency, and attributes higher learning rates to the parameters that are not updated frequently. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-QPbcWlhgfv"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhG3B5g_hgfv"
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTjETKekhgfv"
      },
      "source": [
        "Accuracy function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PfIz45Vhgfv"
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoeKrbqThgfv"
      },
      "source": [
        "Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNPu3ug5hgfv"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        text, text_lengths = batch.text\n",
        "        \n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMtJMBcahgfw"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            text, text_lengths = batch.text\n",
        "            \n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_jyPOvvhgfx"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su0376zvhgfx"
      },
      "source": [
        "Training. Adjust the number of epochs as required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbrWpNshhgfy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89af9e09-6f47-4843-b096-55e426f80483"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 30s\n",
            "\tTrain Loss: 0.674 | Train Acc: 57.81%\n",
            "\t Val. Loss: 0.667 |  Val. Acc: 55.51%\n",
            "Epoch: 02 | Epoch Time: 0m 30s\n",
            "\tTrain Loss: 0.694 | Train Acc: 53.91%\n",
            "\t Val. Loss: 0.679 |  Val. Acc: 53.99%\n",
            "Epoch: 03 | Epoch Time: 0m 30s\n",
            "\tTrain Loss: 0.653 | Train Acc: 61.87%\n",
            "\t Val. Loss: 0.583 |  Val. Acc: 69.79%\n",
            "Epoch: 04 | Epoch Time: 0m 30s\n",
            "\tTrain Loss: 0.583 | Train Acc: 69.60%\n",
            "\t Val. Loss: 0.470 |  Val. Acc: 77.83%\n",
            "Epoch: 05 | Epoch Time: 0m 30s\n",
            "\tTrain Loss: 0.449 | Train Acc: 80.08%\n",
            "\t Val. Loss: 0.348 |  Val. Acc: 86.17%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqjkToDehgfz"
      },
      "source": [
        "Test acccuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq2TqkuVhgfz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a29e2f5-1c82-4455-8534-86aa6f049dc5"
      },
      "source": [
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.363 | Test Acc: 85.05%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeM-dcXDhgf0"
      },
      "source": [
        "## Predictions\n",
        "\n",
        "We can test our model to perform sentiment analysis on input sentences. It will work the best when we give movie reviews as inputs, seeing that it was trained on the IMDb dataset. Put the model in eval mode for inference. \n",
        "\n",
        "\n",
        "Negative reviews will give a value close to zero, while positive reviews will give a value close to one.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYnpcl2Uhgf0"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "    return prediction.item()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RpHoTLehgf0"
      },
      "source": [
        "Example of a negative review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJv9JzrIhgf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdee999d-cf9f-45cc-b51c-24c5ef157b43"
      },
      "source": [
        "predict_sentiment(model, \"This film is terrible\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.02058650366961956"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1ACArvGhgf3"
      },
      "source": [
        "Example of a postive review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV_iPQWjhgf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18548971-cacc-433c-f930-1e7ae12df60b"
      },
      "source": [
        "predict_sentiment(model, \"This film is great\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9379201531410217"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}